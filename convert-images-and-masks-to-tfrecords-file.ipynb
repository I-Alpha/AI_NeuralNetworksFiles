{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"convert-images-and-masks-to-tfrecords-file.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"V2gf1VLZvKoL","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"0ed30743-a817-424d-b2f5-61fd81058b5d","_uuid":"7b05592e9c8c363a82eb523d5f83195e733c5024","id":"ji4ID0jKvTIw","colab_type":"code","outputId":"7226f9ba-c731-4905-93b9-45dab81902d8","executionInfo":{"status":"ok","timestamp":1566839360395,"user_tz":-60,"elapsed":75189,"user":{"displayName":"Ibro","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCBXeeaS85i4GLL0cm2whNVCKv86BDmtWVzPdXP=s64","userId":"17286010715171228163"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["#unpack dataset and organize into 80% train and 20% test in required folder structure\n","%cd /content\n","!mkdir images\n","!mkdir data\n","!mkdir TfRecordfiles\n","!mkdir unused_images\n","\n","%cd /content/data\n","!mkdir train\n","!mkdir test\n","!tar -xf \"/content/drive/My Drive/My Files/Project 2019 Data/images.tar\"  -C /content/images\n","\n","%cd /content/data/train\n","!mkdir masks\n","!mkdir images\n","\n","%cd /content/data/test\n","!mkdir masks\n","!mkdir images\n","\n","%cd /content/data/train/masks\n","!mkdir Lateral\n","!mkdir Primary\n","!mkdir Seeds\n","!mkdir Segmentation\n","\n","\n","%cd /content/data/test/masks\n","!mkdir Lateral\n","!mkdir Primary\n","!mkdir Seeds\n","!mkdir Segmentation\n","\n","!cp -r /content/images/images/{0000..2140}.png /content/data/train/images\n","!cp -r /content/images/lateral16/{0000..2140}.png /content/data/train/masks/Lateral\n","!cp -r /content/images/primary16/{0000..2140}.png /content/data/train/masks/Primary\n","!cp -r /content/images/seeds/{0000..2140}.png /content/data/train/masks/Seeds\n","!cp -r /content/images/segmentation8/{0000..2140}.png /content/data/train/masks/Segmentation\n","\n","!cp -r /content/images/images/{2141..2675}.png /content/data/test/images\n","!cp -r /content/images/lateral16/{2141..2675}.png /content/data/test/masks/Lateral\n","!cp -r /content/images/primary16/{2141..2675}.png /content/data/test/masks/Primary\n","!cp -r /content/images/seeds/{2141..2675}.png /content/data/test/masks/Seeds\n","!cp -r /content/images/segmentation8/{2141..2675}.png /content/data/test/masks/Segmentation\n","!cp -r /content/images/images/{2676..2695}.png /content/unused_images"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n","/content/data\n","/content/data/train\n","/content/data/test\n","/content/data/train/masks\n","/content/data/test/masks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"izXfegpD6Z53","colab":{}},"source":["import math\n","import os\n","import sys\n","import tensorflow as tf\n","\n","slim = tf.contrib.slim\n","\n","#State the labels filename\n","LABELS_FILENAME = ''\n","#===================================================  Dataset Utils  ===================================================\n","#functions for converter. Don't edit unless you know what you're doing. \n","\n","def int64_feature(values):\n","  \"\"\"Returns a TF-Feature of int64s.\n","\n","  Args:\n","    values: A scalar or list of values.\n","\n","  Returns:\n","    a TF-Feature.\n","  \"\"\"\n","  if not isinstance(values, (tuple, list)):\n","    values = [values]\n","  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n","\n","\n","def bytes_feature(values):\n","  \"\"\"Returns a TF-Feature of bytes.\n","\n","  Args:\n","    values: A string.\n","\n","  Returns:\n","    a TF-Feature.\n","  \"\"\"\n","  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n","\n","\n","def image_to_tfexample(image_data, image_format, height, width, class_id):\n","  return tf.train.Example(features=tf.train.Features(feature={\n","      'image/encoded': bytes_feature(image_data),\n","      'image/format': bytes_feature(image_format),\n","      'image/class/label': int64_feature(class_id),\n","      'image/height': int64_feature(height),\n","      'image/width': int64_feature(width),\n","  }))\n","\n","def write_label_file(labels_to_class_names, dataset_dir,\n","                     filename=LABELS_FILENAME):\n","  \"\"\"Writes a file with the list of class names.\n","\n","  Args:\n","    labels_to_class_names: A map of (integer) labels to class names.\n","    dataset_dir: The directory in which the labels file should be written.\n","    filename: The filename where the class names are written.\n","  \"\"\"\n","  labels_filename = os.path.join(dataset_dir, filename)\n","  with tf.gfile.Open(labels_filename, 'w') as f:\n","    for label in labels_to_class_names:\n","      class_name = labels_to_class_names[label]\n","      f.write('%d:%s\\n' % (label, class_name))\n","\n","\n","def has_labels(dataset_dir, filename=LABELS_FILENAME):\n","  \"\"\"Specifies whether or not the dataset directory contains a label map file.\n","\n","  Args:\n","    dataset_dir: The directory in which the labels file is found.\n","    filename: The filename where the class names are written.\n","\n","  Returns:\n","    `True` if the labels file exists and `False` otherwise.\n","  \"\"\"\n","  return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n","\n","\n","def read_label_file(dataset_dir, filename=LABELS_FILENAME):\n","  \"\"\"Reads the labels file and returns a mapping from ID to class name.\n","\n","  Args:\n","    dataset_dir: The directory in which the labels file is found.\n","    filename: The filename where the class names are written.\n","\n","  Returns:\n","    A map from a label (integer) to class name.\n","  \"\"\"\n","  labels_filename = os.path.join(dataset_dir, filename)\n","  with tf.gfile.Open(labels_filename, 'r') as f:\n","    lines = f.read().decode()\n","  lines = lines.split('\\n')\n","  lines = filter(None, lines)\n","\n","  labels_to_class_names = {}\n","  for line in lines:\n","    index = line.index(':')\n","    labels_to_class_names[int(line[:index])] = line[index+1:]\n","  return labels_to_class_names\n","\n","#=======================================  Conversion Utils  ===================================================\n","def __init__(self):\n","#Create an image reader object for easy reading of the images\n","class ImageReader(object):\n","  \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n","\n","  \n","    # Initializes function that decodes RGB JPEG data.\n","    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n","    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n","\n","  def read_image_dims(self, sess, image_data):\n","    image = self.decode_jpeg(sess, image_data)\n","    return image.shape[0], image.shape[1]\n","\n","  def decode_jpeg(self, sess, image_data):\n","    image = sess.run(self._decode_jpeg,\n","                     feed_dict={self._decode_jpeg_data: image_data})\n","    assert len(image.shape) == 3\n","    assert image.shape[2] == 3\n","    return image\n","\n","\n","def _get_filenames_and_classes(dataset_dir):\n","  \"\"\"Returns a list of filenames and inferred class names.\n","\n","  Args:\n","    dataset_dir: A directory containing a set of subdirectories representing\n","      class names. Each subdirectory should contain PNG or JPG encoded images.\n","\n","  Returns:\n","    A list of image file paths, relative to `dataset_dir` and the list of\n","    subdirectories, representing class names.\n","  \"\"\"\n","  # print 'DATASET DIR:', dataset_dir\n","  # print 'subdir:', [name for name in os.listdir(dataset_dir)]\n","  # dataset_main_folder_list = []\n","  # for name in os.listdir(dataset_dir):\n","  # \tif os.path.isdir(name):\n","  # \t\tdataset_main_folder_list.append(name)\n","  dataset_main_folder_list = [name for name in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir,name))]\n","  dataset_root = os.path.join(dataset_dir, dataset_main_folder_list[0])\n","  directories = [\"/content/data/test/images\"]\n","  class_names = ['Primary','Lateral','Seeds','Segmentation']\n","  for filename in os.listdir(dataset_root):\n","    path = os.path.join(dataset_root, filename)\n","    if os.path.isdir(path):\n","      directories.append(path)\n","      class_names.append(filename)\n","\n","  photo_filenames = []\n","  for directory in directories:\n","    for filename in os.listdir(directory):\n","      path = os.path.join(directory, filename)\n","      photo_filenames.append(path)\n","\n","  return photo_filenames, sorted(class_names)\n","\n","\n","def _get_dataset_filename(dataset_dir, split_name, shard_id, tfrecord_filename, _NUM_SHARDS):\n","  output_filename = '%s_%s_%05d-of-%05d.tfrecord' % (\n","      tfrecord_filename, split_name, shard_id, _NUM_SHARDS)\n","  return os.path.join(dataset_dir, output_filename)\n","\n","\n","def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir, tfrecord_filename, _NUM_SHARDS):\n","  \"\"\"Converts the given filenames to a TFRecord dataset.\n","\n","  Args:\n","    split_name: The name of the dataset, either 'train' or 'validation'.\n","    filenames: A list of absolute paths to png or jpg images.\n","    class_names_to_ids: A dictionary from class names (strings) to ids\n","      (integers).\n","    dataset_dir: The directory where the converted datasets are stored.\n","  \"\"\"\n","  assert split_name in ['train', 'validation']\n","\n","  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n","\n","  with tf.Graph().as_default():\n","    image_reader = ImageReader()\n","\n","    with tf.Session('') as sess:\n","\n","      for shard_id in range(_NUM_SHARDS):\n","        output_filename = _get_dataset_filename(\n","            dataset_dir, split_name, shard_id, tfrecord_filename = tfrecord_filename, _NUM_SHARDS = _NUM_SHARDS)\n","\n","        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n","          start_ndx = shard_id * num_per_shard\n","          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n","          for i in range(start_ndx, end_ndx):\n","            sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n","                i+1, len(filenames), shard_id))\n","            sys.stdout.flush()\n","\n","            # Read the filename:\n","            image_data = tf.gfile.FastGFile(filenames[i], 'r').read()\n","            height, width = image_reader.read_image_dims(sess, image_data)\n","\n","            class_name = os.path.basename(os.path.dirname(filenames[i]))\n","            class_id = class_names_to_ids[class_name]\n","\n","            example = image_to_tfexample(\n","                image_data, 'jpg', height, width, class_id)\n","            tfrecord_writer.write(example.SerializeToString())\n","\n","  sys.stdout.write('\\n')\n","  sys.stdout.flush()\n","\n","def _dataset_exists(dataset_dir, _NUM_SHARDS, output_filename):\n","  for split_name in ['train', 'validation']:\n","    for shard_id in range(_NUM_SHARDS):\n","      tfrecord_filename = _get_dataset_filename(\n","          dataset_dir, split_name, shard_id, output_filename, _NUM_SHARDS)\n","      if not tf.gfile.Exists(tfrecord_filename):\n","        return False\n","  return True\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tcwvjr-r6wjf","colab_type":"code","outputId":"9736f355-3862-427b-b240-9db2c78860aa","executionInfo":{"status":"ok","timestamp":1566840968033,"user_tz":-60,"elapsed":7343,"user":{"displayName":"Ibro","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCBXeeaS85i4GLL0cm2whNVCKv86BDmtWVzPdXP=s64","userId":"17286010715171228163"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["%cd /content\n","!git clone https://github.com/I-Alpha/Mask_RCNN.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'Mask_RCNN'...\n","remote: Enumerating objects: 956, done.\u001b[K\n","remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n","Receiving objects: 100% (956/956), 111.90 MiB | 43.87 MiB/s, done.\n","Resolving deltas: 100% (571/571), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"519501a2-749b-4240-94e2-957f055d2cc9","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"127ad4b7c19e87ff2fb180d1030dbbe4ea430a94","id":"rQOcpLvCvTI9","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os\n","\n","\n","#construct list for image paths and their corresponding masks in the same order\n","#image_path = \"/content/data/images/train/*.png\"\n","dta_type = \"test\" #specifies train or test folder to work on. \n","\n","image_path = []\n","for filename in os.listdir(\"/content/data/\"+dta_type+\"/images\"):\n","    image_path.append(\"/content/data/\"+dta_type+\"/images/\"+filename)\n","\n","Pmask_path = []\n","for i, filename in enumerate(image_path):\n","    Pmask_path.append(\"/content/data/\" + dta_type + \"/masks/Primary/\" + os.path.basename(filename))\n","    \n","Lmask_path = []\n","for i, filename in enumerate(image_path):\n","    Lmask_path.append(\"/content/data/\" + dta_type + \"/masks/Lateral/\" + os.path.basename(filename))\n","\n","Smask_path = []\n","for i, filename in enumerate(image_path):\n","    Smask_path.append(\"/content/data/\" + dta_type + \"/masks/Seeds/\" + os.path.basename(filename))\n","\n","Segmask_path = []\n","for i, filename in enumerate(image_path):\n","    Segmask_path.append(\"/content/data/\" + dta_type + \"/masks/Segmentation/\" + os.path.basename(filename))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"eeff0142-e9c9-40f9-a771-c87e1a757d99","_uuid":"d14d3460a74e19b631dc8571bf21164cf4522444","collapsed":true,"id":"Xk6IBaxlvTJD","colab_type":"code","outputId":"1c908520-51b1-4f3a-836b-06f2f5040f82","executionInfo":{"status":"error","timestamp":1566840973217,"user_tz":-60,"elapsed":524,"user":{"displayName":"Ibro","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCBXeeaS85i4GLL0cm2whNVCKv86BDmtWVzPdXP=s64","userId":"17286010715171228163"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["from PIL import Image\n","# path to where the tfrecords will be stored (change to your customized path).\n","# I can't run this here in Kaggle because of permission. \n","# Any comment on how to write temporary files in Kaggle will be appreciated.\n","tfrecords_filename = \"/content/TfRecordfiles/\"+ dta_type + \"ing_data.tfrecords\"\n","#get a writer for the tfrecord file.\n","writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n","#write data/masks into tfrecords\n","for i in range(len(image_path)):\n","    print(i+1,len(image_path)-1)\n","    img = Image.open(image_path[i]).convert('RGB')\n","    img = np.array(img)\n","    \n","    Pmask = np.array(mpimg.imread(Pmask_path[i]),np.dtype(np.uint8)) \n","    Lmask = np.array(mpimg.imread(Lmask_path[i]),np.dtype(np.uint8))\n","   #Smask = np.array(mpimg.imread(Smask_path[i]),np.dtype(np.uint8))   Smasks are 24-bit.Have to convert.\n","    Smask = Image.open(Smask_path[i]).convert('L')\n","    Smask = np.array(Smask,np.dtype(np.uint8))\n","    #print((Smask))\n","    Segmask = np.array(mpimg.imread(Segmask_path[i]),np.dtype(np.uint8))\n","    \n","    \n","    height = img.shape[0]\n","    width = img.shape[1]\n","    img_raw = img.tostring()\n","    Pmask_raw = Pmask.tostring()\n","    Lmask_raw = Lmask.tostring()\n","    Smask_raw = Smask.tostring()\n","    Segmask_raw = Segmask.tostring()\n","    \n","    #save the heights and widths as well so, which \n","    #are needed when decoding from tfrecords back to images\n","    example = tf.train.Example(features=tf.train.Features(feature={\n","                                                          'height': _int64_feature(height),\n","                                                          'width': _int64_feature(width),\n","                                                          'image_raw':bytes_feature(img_raw),\n","                                                          'Pmask_raw': _bytes_feature(Pmask_raw),\n","                                                          'Lmask_raw': _bytes_feature(Lmask_raw),\n","                                                          'Smask_raw': _bytes_feature(Smask_raw),\n","                                                          'Segmask_raw': _bytes_feature(Segmask_raw)}\n","                                                          ))\n","    writer.write(example.SerializeToString())\n","writer.close()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 534\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-8a0307d3c9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#are needed when decoding from tfrecords back to images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     example = tf.train.Example(features=tf.train.Features(feature={\n\u001b[0;32m---> 34\u001b[0;31m                                                           \u001b[0;34m'height'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_int64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                                                           \u001b[0;34m'width'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_int64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                           \u001b[0;34m'image_raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbytes_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_int64_feature' is not defined"]}]},{"cell_type":"code","metadata":{"_cell_guid":"c576608e-3b8d-4ead-8bb6-a83c402c6bf1","_uuid":"ac14b9fd2086f4c76f68f7a7d32d58e33ca9a625","collapsed":true,"id":"CBdFZlePvTJL","colab_type":"code","outputId":"3651ff67-f88a-4d36-b8ac-9dc594454139","executionInfo":{"status":"error","timestamp":1566786264701,"user_tz":-60,"elapsed":110598,"user":{"displayName":"Ibro","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCBXeeaS85i4GLL0cm2whNVCKv86BDmtWVzPdXP=s64","userId":"17286010715171228163"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16kOepTrXJcI41PyPuR5jOfjkxnQQkiEL"}},"source":["#run the following to verify the created tfrecord file.\n","record_iterator = tf.io.tf_record_iterator(tfrecords_filename)\n","xcount = 1\n","for string_record in record_iterator:\n","    \n","    example = tf.train.Example()\n","    example.ParseFromString(string_record)\n","    \n","    #get features \n","    height = int(example.features.feature['height'].int64_list.value[0])\n","    width = int(example.features.feature['width'].int64_list.value[0])\n","    img_string = (example.features.feature['image_raw'].bytes_list.value[0])\n","    Pmask_string = (example.features.feature['Pmask_raw'].bytes_list.value[0])\n","    Lmask_string = (example.features.feature['Lmask_raw'].bytes_list.value[0])\n","    Smask_string = (example.features.feature['Smask_raw'].bytes_list.value[0])\n","    Segmask_string = (example.features.feature['Segmask_raw'].bytes_list.value[0])\n","    \n","    #get 1d array from string of features\n","    img_1d = np.frombuffer(img_string, dtype=np.uint8)\n","    Pmask_1d = np.frombuffer(Pmask_string, dtype=np.uint8)\n","    Lmask_1d = np.frombuffer(Lmask_string, dtype=np.uint8)\n","    Smask_1d = np.frombuffer(Smask_string, dtype=np.uint8)\n","    Segmask_1d = np.frombuffer(Segmask_string, dtype=np.uint8)\n","\n","    #reshape back to their original shape from a 1D array read from tfrecords\n","    img = img_1d.reshape((height, width, -1))\n","    Pmask = Pmask_1d.reshape((height, width))\n","    Lmask = Lmask_1d.reshape((height, width))\n","    Smask = Smask_1d.reshape((height, width))\n","    Segmask = Segmask_1d.reshape((height, width))\n","    \n","  \n","    print(\"image : \",xcount)\n","    xcount+=1\n","    plt.imshow(img)\n","    plt.show()\n","    plt.imshow(Pmask)\n","    plt.show()\n","    plt.imshow(Lmask)\n","    plt.show()\n","    plt.imshow(Smask)\n","    plt.show()\n","    plt.imshow(Segmask)\n","    plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Yo_NtloUiNEC","colab_type":"code","colab":{}},"source":["%cd /content \n","!git clone https://github.com/matterport/Mask_RCNN.git\n","/content/drive/My Drive/My Files/Project 2019 Data/Labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLNBAqO4IRxm","colab_type":"code","colab":{}},"source":["\n","\n","Labels_map = {0: {'id': 0, 'name': 'Primary'},\n","              1: {'id': 1, 'name': 'Lateral'},\n","              2: {'id': 2, 'name': 'Seeds'},\n","              3: {'id': 3, 'name': 'Segmentation'}}\n","\n","train_dataset = tf.data.TFRecordDataset(\"/content/TfRecordfiles/testing_data.tfrecords\")\n","test_dataset = tf.data.TFRecordDataset(\"/content/TfRecordfiles/training_data.tfrecords\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uP4iFYaTPi3J","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}